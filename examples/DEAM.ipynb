{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Deam dataset is based on Arousal-Valence 2D emotional model.\n",
    "The Valence/Arousal ratings were collected using Amazon Mechanical Turks service.\n",
    "Each turk from the collected crowd were asked to mark his own emotion for the current song on a 2D plane, Arousal/Valence.\n",
    "For more information please read: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0173392\n",
    "\"\"\"\n",
    "import os\n",
    "import torch\n",
    "import pandas\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from torch.optim import SGD\n",
    "from torch.nn import MSELoss\n",
    "from torch.utils.data import DataLoader\n",
    "from config import DATA_DIR, MODELS_DIR\n",
    "from src.model import MF, SingleMF\n",
    "from src.runner import SingleMFRunner\n",
    "from src.utils import (\n",
    "    create_dataset,\n",
    "    mine_outliers_scipy,\n",
    "    mine_outliers_sklearn,\n",
    "    DataConverter,\n",
    ")\n",
    "from tensorboardX import SummaryWriter\n",
    "from src.consistency import direct_consistency_calculation, mf_consistency_calculation\n",
    "from sklearn.manifold import TSNE\n",
    "from pandas import DataFrame\n",
    "from src.utils import ProcColumn\n",
    "from src.runner import Runner\n",
    "from src.data_set import RatingsDataset\n",
    "from collections import namedtuple\n",
    "from src.consistency import clac_cronbach_alpha\n",
    "from colorama import Fore, Style\n",
    "\n",
    "Row = namedtuple(\"Row\", \"workerID SongId Valence Arousal Emotion\")\n",
    "\n",
    "# experiment = \"mean_centralised\"\n",
    "# experiment = \"standardized\"\n",
    "experiment = \"raw\"\n",
    "include_bias = experiment == \"raw\"\n",
    "\n",
    "DF_PATH = f\"{DATA_DIR}\" \\\n",
    "          f\"/DEAM/annotations/annotations per each rater/\" \\\n",
    "          f\"song_level/static_annotations_songs_1_2000_{experiment}.csv\"\n",
    "\n",
    "\n",
    "def get_emotion(valence: int, arousal: int, valence_mean: float, arousal_mean: float) -> int:\n",
    "    \"\"\"\n",
    "    Selects emotion based on Valence/Arousal.\n",
    "    \"\"\"\n",
    "    if arousal <= arousal_mean and valence <= valence_mean:\n",
    "        return 3\n",
    "\n",
    "    if arousal >= arousal_mean and valence <= valence_mean:\n",
    "        return 2\n",
    "\n",
    "    if arousal <= arousal_mean and valence >= valence_mean:\n",
    "        return 4\n",
    "\n",
    "    if arousal >= arousal_mean and valence >= valence_mean:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "def select_n_random(trainset: RatingsDataset):\n",
    "    \"\"\"\n",
    "    Selects n random data points and their corresponding labels from a dataset\n",
    "    \"\"\"\n",
    "    perm = torch.randperm(len(trainset))\n",
    "    return trainset[perm][:100]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "                           workerID  SongId  Valence  Arousal\n0  6010bbc8e7ef4b21fa38f9c3a9754ef3       2        5        2\n1  3c888e77b992ae3cd2adfe16774e23b9       2        2        3\n2  2afd218c3aecb6828d2be327f8b9c46f       2        3        3\n3  fd5b08ce362d855ca9152a894348130c       2        4        4\n4  9c8073214a052e414811b76012df8847       2        2        2",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>workerID</th>\n      <th>SongId</th>\n      <th>Valence</th>\n      <th>Arousal</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6010bbc8e7ef4b21fa38f9c3a9754ef3</td>\n      <td>2</td>\n      <td>5</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3c888e77b992ae3cd2adfe16774e23b9</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2afd218c3aecb6828d2be327f8b9c46f</td>\n      <td>2</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>fd5b08ce362d855ca9152a894348130c</td>\n      <td>2</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9c8073214a052e414811b76012df8847</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"workerID\", \"SongId\", \"Valence\", \"Arousal\"]\n",
    "original_df = pandas.read_csv(DF_PATH, skipinitialspace=True, usecols=columns)\n",
    "original_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This block of code calculates the outliers alongside the valence axis\n",
    "\"\"\"\n",
    "valence_df = original_df[[\"workerID\", \"SongId\", \"Valence\"]]\n",
    "valence_df.columns = [\"user_id\", \"item_id\", \"rating\"]\n",
    "\n",
    "valence_data_converter = DataConverter(original_df=valence_df)\n",
    "\n",
    "valence_model = MF(\n",
    "        n_users=valence_data_converter.n_users,\n",
    "        n_items=valence_data_converter.n_item,\n",
    "        include_bias=include_bias,\n",
    "        n_factors=300,\n",
    ")\n",
    "if os.path.exists(f\"{MODELS_DIR}/DEAM/{experiment}/valence.pt\"):\n",
    "    valence_model.load_state_dict(torch.load(f\"{MODELS_DIR}/DEAM/{experiment}/valence.pt\"))\n",
    "else:\n",
    "    epochs = 300\n",
    "\n",
    "    criterion = MSELoss()\n",
    "    optimizer = SGD(valence_model.parameters(), lr=5, weight_decay=1e-3)\n",
    "    runner = Runner(\n",
    "        model=valence_model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        epochs=epochs\n",
    "    )\n",
    "\n",
    "    train_set = create_dataset(data_frame=valence_data_converter.encoded_df)\n",
    "    train_load = DataLoader(train_set, batch_size=1000, shuffle=True)\n",
    "    users, items, ratings = select_n_random(train_set)\n",
    "\n",
    "    with SummaryWriter(f\"runs/DEAM/{experiment}/valence\") as writer:\n",
    "        writer.add_graph(valence_model, (users, items))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = runner.train(train_loader=train_load, epoch=epoch, writer=writer)\n",
    "            print(f\"epoch={epoch + 1}, loss={epoch_loss}\")\n",
    "\n",
    "    torch.save(valence_model.state_dict(), f\"{MODELS_DIR}/DEAM/{experiment}/valence.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This block of code calculates the outliers alongside the Arousal axis\n",
    "\"\"\"\n",
    "arousal_df = original_df[[\"workerID\", \"SongId\", \"Arousal\"]]\n",
    "arousal_df.columns = [\"user_id\", \"item_id\", \"rating\"]\n",
    "\n",
    "arousal_data_converter = DataConverter(original_df=arousal_df)\n",
    "\n",
    "arousal_model = MF(\n",
    "        n_users=arousal_data_converter.n_users,\n",
    "        n_items=arousal_data_converter.n_item,\n",
    "        include_bias=include_bias,\n",
    "        n_factors=300,\n",
    ")\n",
    "if os.path.exists(f\"{MODELS_DIR}/DEAM/{experiment}/arousal.pt\"):\n",
    "    arousal_model.load_state_dict(torch.load(f\"{MODELS_DIR}/DEAM/{experiment}/arousal.pt\"))\n",
    "else:\n",
    "    epochs = 300\n",
    "\n",
    "    criterion = MSELoss()\n",
    "    optimizer = SGD(arousal_model.parameters(), lr=5, weight_decay=1e-3)\n",
    "    runner = Runner(\n",
    "        model=arousal_model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        epochs=epochs,\n",
    "    )\n",
    "\n",
    "    train_set = create_dataset(data_frame=arousal_data_converter.encoded_df)\n",
    "    train_load = DataLoader(train_set, batch_size=1000, shuffle=True)\n",
    "    users, items, ratings = select_n_random(train_set)\n",
    "\n",
    "    with SummaryWriter(f\"runs/DEAM/{experiment}/arousal\") as writer:\n",
    "        writer.add_graph(arousal_model, (users, items))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = runner.train(train_loader=train_load, epoch=epoch, writer=writer)\n",
    "            print(f\"epoch={epoch + 1}, loss={epoch_loss}\")\n",
    "\n",
    "    torch.save(arousal_model.state_dict(), f\"{MODELS_DIR}/DEAM/{experiment}/arousal.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'2a6b63b7690efa2390c8d9fee11b1407',\n",
      "'ad3b997c4f2382a66e49f035cacfa682',\n",
      "'65794ea9f5122952403585a237bc5e52',\n",
      "'fd5b08ce362d855ca9152a894348130c',\n",
      "'374a5659c02e12b01db6319436f17a7d',\n",
      "'bb50b45a1874ede476874bd57e4cabb4',\n",
      "'485d8e33a731a830ef0aebd71b016d08',\n",
      "'615d836ba25132081e0ebd2182221a59',\n",
      "'da37d1548ffd0631809f7be341e4fe4d',\n",
      "'a30d244141cb2f51e0803e79bc4bd147',\n",
      "\\begin{tabular}{lllr}\n",
      "\\toprule\n",
      "{} & user\\_id & cosine\\_similarity &  annotations \\\\\n",
      "\\midrule\n",
      "1  &   2a6b6 &             12.04 &            3 \\\\\n",
      "2  &   ad3b9 &             19.67 &            3 \\\\\n",
      "3  &   65794 &             19.96 &            3 \\\\\n",
      "4  &   fd5b0 &             20.85 &          222 \\\\\n",
      "5  &   374a5 &             21.09 &            3 \\\\\n",
      "6  &   bb50b &             31.69 &          178 \\\\\n",
      "7  &   485d8 &             34.89 &            6 \\\\\n",
      "8  &   615d8 &             35.81 &            6 \\\\\n",
      "9  &   da37d &             38.08 &            3 \\\\\n",
      "10 &   a30d2 &             41.99 &          985 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tf/cy7jlqss4qs8lktmfhln3y5w0000gq/T/ipykernel_13958/3764702803.py:26: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(outliers.to_latex())\n"
     ]
    }
   ],
   "source": [
    "valence_embeddings = list(valence_model.user_factors.parameters())[0].detach().cpu()\n",
    "valence_similarities = mine_outliers_sklearn(embeddings=valence_embeddings)\n",
    "valence_outliers = {valence_data_converter.get_original_user_id(i): score for i, score in enumerate(valence_similarities)}\n",
    "\n",
    "arousal_embeddings = list(arousal_model.user_factors.parameters())[0].detach().cpu()\n",
    "arousal_similarities = mine_outliers_sklearn(embeddings=arousal_embeddings)\n",
    "arousal_outliers = {arousal_data_converter.get_original_user_id(i): score for i, score in enumerate(arousal_similarities)}\n",
    "\n",
    "\n",
    "items_group_by_users = valence_data_converter.original_df.groupby(\"user_id\")\n",
    "combined_outliers = {}\n",
    "for user_id, valence_dist in valence_outliers.items():\n",
    "    arousal_dist = arousal_outliers[user_id]\n",
    "    combined_outliers[user_id] = valence_dist + arousal_dist\n",
    "\n",
    "Outlier = namedtuple(\"Outlier\", [\"user_id\" ,\"cosine_similarity\" ,\"annotations\"])\n",
    "combined_outliers = dict(sorted(combined_outliers.items(), key=lambda item: item[1])[:10])\n",
    "outliers = []\n",
    "for user_id, dist in combined_outliers.items():\n",
    "    print(f\"'{user_id}',\")\n",
    "    number_of_items = len(items_group_by_users.get_group(user_id))\n",
    "    outliers.append(Outlier(user_id=user_id[:5], cosine_similarity=\"{:.2f}\".format(dist), annotations=number_of_items))\n",
    "\n",
    "outliers = pandas.DataFrame.from_dict(outliers)\n",
    "outliers.index += 1\n",
    "print(outliers.to_latex())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1, loss=4.308509826660156\n",
      "epoch=101, loss=0.07582364082336426\n",
      "epoch=201, loss=0.04621601998806\n",
      "epoch=301, loss=0.02899077534675598\n",
      "epoch=401, loss=0.018272922933101655\n",
      "epoch=501, loss=0.011526497453451157\n",
      "epoch=601, loss=0.007271832972764969\n",
      "epoch=701, loss=0.004587733745574951\n",
      "epoch=801, loss=0.0028943827375769613\n",
      "epoch=901, loss=0.0018260590732097626\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This block of code inserting new outlier user into the dataset, and then tries to optimize the new user embedding according\n",
    "to the optimized item embedding of the model.\n",
    "We want to check if we can detect outlier in post training phase.\n",
    "This block works on the Valence axis\n",
    "\"\"\"\n",
    "\n",
    "valence_single_mf_model = SingleMF(optimized_item_factors=valence_model.user_factors, n_factors=300)\n",
    "criterion = MSELoss()\n",
    "optimizer = SGD(valence_single_mf_model.parameters(), lr=.1, weight_decay=1e-7)\n",
    "outlier_dataframe = valence_data_converter.create_outlier_dataset(\n",
    "    original_df=valence_df, number_of_users_to_add=1, n_ratings_per_random_user=10\n",
    ")\n",
    "\n",
    "outlier_data_converter = DataConverter(original_df=outlier_dataframe)\n",
    "outlier_dataset = create_dataset(data_frame=outlier_data_converter.encoded_df)\n",
    "outlier_train_load = DataLoader(outlier_dataset, batch_size=len(outlier_dataset), shuffle=True)\n",
    "\n",
    "single_mf_runner = SingleMFRunner(\n",
    "    model=valence_single_mf_model, criterion=criterion, optimizer=optimizer\n",
    ")\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = single_mf_runner.train(train_loader=outlier_train_load)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"epoch={epoch + 1}, loss={epoch_loss}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1, loss=3.8076892852783204\n",
      "epoch=101, loss=0.023093771934509278\n",
      "epoch=201, loss=0.008550535887479782\n",
      "epoch=301, loss=0.00434623584151268\n",
      "epoch=401, loss=0.0025340836495161057\n",
      "epoch=501, loss=0.0015426399186253548\n",
      "epoch=601, loss=0.0009504844434559345\n",
      "epoch=701, loss=0.0005875375121831894\n",
      "epoch=801, loss=0.0003635013941675425\n",
      "epoch=901, loss=0.00022494681179523467\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This block of code inserting new outlier user into the dataset, and then tries to optimize the new user embedding according\n",
    "to the optimized item embedding of the model.\n",
    "We want to check if we can detect outlier in post training phase.\n",
    "This block works on the Arousal axis\n",
    "\"\"\"\n",
    "\n",
    "arousal_single_mf_model = SingleMF(optimized_item_factors=arousal_model.user_factors, n_factors=300)\n",
    "criterion = MSELoss()\n",
    "optimizer = SGD(arousal_single_mf_model.parameters(), lr=.1, weight_decay=1e-7)\n",
    "outlier_dataframe = arousal_data_converter.create_outlier_dataset(\n",
    "    original_df=arousal_df, number_of_users_to_add=1, n_ratings_per_random_user=10\n",
    ")\n",
    "\n",
    "outlier_data_converter = DataConverter(original_df=outlier_dataframe)\n",
    "outlier_dataset = create_dataset(data_frame=outlier_data_converter.encoded_df)\n",
    "outlier_train_load = DataLoader(outlier_dataset, batch_size=len(outlier_dataset), shuffle=True)\n",
    "\n",
    "single_mf_runner = SingleMFRunner(\n",
    "    model=arousal_single_mf_model, criterion=criterion, optimizer=optimizer\n",
    ")\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = single_mf_runner.train(train_loader=outlier_train_load)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"epoch={epoch + 1}, loss={epoch_loss}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllr}\n",
      "\\toprule\n",
      "{} &           user\\_id & cosine\\_similarity &  annotations \\\\\n",
      "\\midrule\n",
      "1  &             2a6b6 &             12.12 &            3 \\\\\n",
      "2  &             ad3b9 &             19.60 &            3 \\\\\n",
      "3  &             65794 &             20.01 &            3 \\\\\n",
      "4  &             fd5b0 &             20.96 &          222 \\\\\n",
      "5  &             374a5 &             21.01 &            3 \\\\\n",
      "6  &  random annotator &             23.33 &           10 \\\\\n",
      "7  &             bb50b &             31.59 &          178 \\\\\n",
      "8  &             485d8 &             34.93 &            6 \\\\\n",
      "9  &             615d8 &             35.84 &            6 \\\\\n",
      "10 &             da37d &             38.04 &            3 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tf/cy7jlqss4qs8lktmfhln3y5w0000gq/T/ipykernel_13958/854381871.py:43: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(outliers.to_latex())\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This block add the new outlier user embeddings into the existing user embeddings and tries to detect whether his outlier or not.\n",
    "\"\"\"\n",
    "\n",
    "valence_embeddings = list(valence_model.user_factors.parameters())[0].detach().cpu()\n",
    "outlier_valence_embeddings = list(valence_single_mf_model.user_factors.parameters())[0].detach().cpu()\n",
    "valence_embeddings = torch.cat((valence_embeddings, outlier_valence_embeddings), 0)\n",
    "\n",
    "arousal_embeddings = list(arousal_model.user_factors.parameters())[0].detach().cpu()\n",
    "outlier_arousal_embeddings = list(arousal_single_mf_model.user_factors.parameters())[0].detach().cpu()\n",
    "arousal_embeddings = torch.cat((arousal_embeddings, outlier_arousal_embeddings), 0)\n",
    "\n",
    "valence_similarities = mine_outliers_sklearn(embeddings=valence_embeddings)\n",
    "arousal_similarities = mine_outliers_sklearn(embeddings=arousal_embeddings)\n",
    "\n",
    "combined_outliers = {}\n",
    "for i, (valence_dist, arousal_dist) in enumerate(zip(valence_similarities, arousal_similarities)):\n",
    "    if i == len(arousal_similarities) - 1:\n",
    "        outlier_id = outlier_data_converter.get_original_user_id(encoded_id=0)\n",
    "        combined_outliers[outlier_id] = valence_dist + arousal_dist\n",
    "        continue\n",
    "\n",
    "    user_id = valence_data_converter.get_original_user_id(encoded_id=i)\n",
    "    combined_outliers[user_id] = valence_dist + arousal_dist\n",
    "\n",
    "items_group_by_users = valence_data_converter.original_df.groupby(\"user_id\")\n",
    "outlier_items_group_by_users = outlier_data_converter.original_df.groupby(\"user_id\")\n",
    "\n",
    "combined_outliers = dict(sorted(combined_outliers.items(), key=lambda item: item[1])[:10])\n",
    "outliers = []\n",
    "for user_id, dist in combined_outliers.items():\n",
    "    try:\n",
    "        number_of_items = len(items_group_by_users.get_group(user_id))\n",
    "        outliers.append(Outlier(user_id=user_id[:5], cosine_similarity=\"{:.2f}\".format(dist), annotations=number_of_items))\n",
    "        # print(f\"user: {user_id}, dist: {dist}, #items: {number_of_items}\")\n",
    "    except KeyError:\n",
    "        # handle outlier\n",
    "        number_of_items = len(outlier_items_group_by_users.get_group(user_id))\n",
    "        outliers.append(Outlier(user_id=user_id, cosine_similarity=\"{:.2f}\".format(dist), annotations=number_of_items))\n",
    "        # print(f\"user: {user_id}, dist: {dist}, #items: {10}\")\n",
    "outliers = pandas.DataFrame.from_dict(outliers)\n",
    "outliers.index += 1\n",
    "print(outliers.to_latex())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/17464 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9945ffbd4b164c2f90004f779da4b54c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/17464 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "139c1ca36c894f4d967588586bd06620"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data consistency with outliers according to direct calculation is: \u001B[33m-4.4853010194856324e-14\u001B[32m\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16429 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ac3958299724c0eb070294e94526415"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16429 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2727c11d68184b3d9dc1bd6314a49071"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data consistency without outliers according to direct calculation is: \u001B[43m-1.0091927293842673e-13\u001B[42m\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This block analyze raw data consistency using the direct calculation defined by:\n",
    "consistency += row.rating - row.song.mean() for all rows in dataset.\n",
    "In addition we try to identify the consistency in the dataset after dropping the outliers using the direct calculation.\n",
    "\"\"\"\n",
    "\n",
    "outliers_names = combined_outliers.keys()\n",
    "\n",
    "valence_consistency = direct_consistency_calculation(data_frame=valence_data_converter.original_df)\n",
    "arousal_consistency = direct_consistency_calculation(data_frame=arousal_data_converter.original_df)\n",
    "\n",
    "print(f\"Raw data consistency with outliers according to direct calculation is: \\x1b[33m{valence_consistency + arousal_consistency}\\x1b[32m\")\n",
    "\n",
    "valence_df_without_outliers = valence_df[~valence_df.user_id.isin(outliers_names)]\n",
    "arousal_df_without_outliers = arousal_df[~arousal_df.user_id.isin(outliers_names)]\n",
    "\n",
    "valence_consistency = direct_consistency_calculation(data_frame=valence_df_without_outliers)\n",
    "arousal_consistency = direct_consistency_calculation(data_frame=arousal_df_without_outliers)\n",
    "\n",
    "print(f\"Raw data consistency without outliers according to direct calculation is: \\x1b[43m{valence_consistency + arousal_consistency}\\x1b[42m\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mf_calculation: 100%|██████████| 17464/17464 [00:02<00:00, 7943.78it/s]\n",
      "mf_calculation: 100%|██████████| 17464/17464 [00:02<00:00, 8457.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data consistency with outliers according to matrix factorization calculation is: \u001B[33m6708.077201962471\u001B[32m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mf_calculation: 100%|██████████| 17464/17464 [00:02<00:00, 7718.31it/s]\n",
      "mf_calculation: 100%|██████████| 17464/17464 [00:01<00:00, 9149.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data consistency without outliers according to matrix factorization calculation is: \u001B[41m6367.561932563782\u001B[42m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This block tries to identify the consistency in the dataset after MF.\n",
    "The mf consistency is defined by:\n",
    "consistency += row.rating - model.prediction(row.user, row.item) for all rows in dataset.\n",
    "First we are trying to identify the consistency with outliers, afterwards we are removing\n",
    "the outliers and re-run the calculation.\n",
    "\"\"\"\n",
    "\n",
    "valence_consistency = mf_consistency_calculation(data_frame=valence_df, model=valence_model, outliers={}, round_prediction=False)\n",
    "arousal_consistency = mf_consistency_calculation(data_frame=arousal_df, model=arousal_model, outliers={}, round_prediction=False)\n",
    "\n",
    "print(f\"Raw data consistency with outliers according to matrix factorization calculation is: \\x1b[33m{valence_consistency + arousal_consistency}\\x1b[32m\")\n",
    "\n",
    "valence_consistency = mf_consistency_calculation(data_frame=valence_df, model=valence_model, outliers=combined_outliers, round_prediction=False)\n",
    "arousal_consistency = mf_consistency_calculation(data_frame=arousal_df, model=arousal_model, outliers=combined_outliers, round_prediction=False)\n",
    "\n",
    "print(f\"Raw data consistency without outliers according to matrix factorization calculation is: \\x1b[41m{valence_consistency + arousal_consistency}\\x1b[42m\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "def to_mf_df():\n",
    "    working_df = pandas.read_csv(DF_PATH, skipinitialspace=True, usecols=columns)\n",
    "    valence_mean = valence_df.rating.mean()\n",
    "    arousal_mean = arousal_df.rating.mean()\n",
    "\n",
    "    user_original_id_to_encoded_id = ProcColumn(working_df.workerID)\n",
    "    item_original_id_to_encoded_id = ProcColumn(working_df.SongId)\n",
    "    working_df.workerID = user_original_id_to_encoded_id.encoded_col\n",
    "    working_df.SongId = item_original_id_to_encoded_id.encoded_col\n",
    "\n",
    "    df = []\n",
    "    for (index, worker_id, song_id, valence, arousal) in working_df.itertuples():\n",
    "        user_id_as_tensor = torch.LongTensor([worker_id])\n",
    "        item_id_as_tensor = torch.LongTensor([song_id])\n",
    "        with torch.no_grad():\n",
    "            valence_prediction = valence_model(\n",
    "                users=user_id_as_tensor, items=item_id_as_tensor,\n",
    "            ).item()\n",
    "            arousal_prediction = arousal_model(\n",
    "                users=user_id_as_tensor, items=item_id_as_tensor,\n",
    "            ).item()\n",
    "            emotion_predicted = get_emotion(\n",
    "                valence=valence_prediction,\n",
    "                arousal=arousal_prediction,\n",
    "                valence_mean=valence_mean,\n",
    "                arousal_mean=arousal_mean,\n",
    "            )\n",
    "\n",
    "        original_worker_id = user_original_id_to_encoded_id.get_name(index=worker_id)\n",
    "        original_item_id = item_original_id_to_encoded_id.get_name(index=song_id)\n",
    "        df.append(\n",
    "            Row(\n",
    "                workerID=original_worker_id,\n",
    "                SongId=original_item_id,\n",
    "                Valence=valence_prediction,\n",
    "                Arousal=arousal_prediction,\n",
    "                Emotion=emotion_predicted,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    df = DataFrame(df, columns=[\"workerID\", \"SongId\", \"Valence\", \"Arousal\", \"Emotion\"])\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def k_means(dataframe: DataFrame, title: str):\n",
    "    mat = dataframe[[\"Valence\", \"Arousal\"]]\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(mat)\n",
    "    scaled_features = DataFrame(scaled_features)\n",
    "    scaled_features.columns = [\"Valence\", \"Arousal\"]\n",
    "    scaled_features[\"Emotion\"] = dataframe.Emotion\n",
    "    km = KMeans(n_clusters=4)\n",
    "    y_km = km.fit_predict(scaled_features).astype(str)\n",
    "    fig = px.scatter(\n",
    "        x=scaled_features[\"Arousal\"],\n",
    "        y=scaled_features[\"Valence\"],\n",
    "        color_discrete_sequence=px.colors.qualitative.G10,\n",
    "        color=y_km,\n",
    "        hover_data={\"Item id\": original_df.SongId.values},\n",
    "        title=title,\n",
    "    )\n",
    "    fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def plot_embeddings(dataframe: DataFrame):\n",
    "    valence_item_embeddings = list(valence_model.item_factors.parameters())[0].detach().cpu()\n",
    "    arousal_item_embeddings = list(arousal_model.item_factors.parameters())[0].detach().cpu()\n",
    "\n",
    "    valence_item_embeddings = np.array(valence_item_embeddings)\n",
    "    valence_tsne = TSNE(n_components=2, random_state=0).fit_transform(valence_item_embeddings)\n",
    "    km = KMeans(n_clusters=2)\n",
    "    v_y_km = km.fit_predict(valence_tsne).astype(str)\n",
    "\n",
    "    arousal_item_embeddings = np.array(arousal_item_embeddings)\n",
    "    arousal_tsne = TSNE(n_components=2, random_state=0).fit_transform(arousal_item_embeddings)\n",
    "    a_y_km = km.fit_predict(arousal_tsne).astype(str)\n",
    "\n",
    "    fig = px.scatter(\n",
    "        valence_tsne,\n",
    "        x=0,\n",
    "        y=1,\n",
    "        color_discrete_sequence=px.colors.qualitative.G10,\n",
    "        color=v_y_km,\n",
    "        labels={\"color\": \"cluster\"},\n",
    "        hover_data={\"song_id\": dataframe.SongId.unique()},\n",
    "        title=\"Valence Embeddings\"\n",
    "    )\n",
    "    fig.update_traces(marker_size=8)\n",
    "    fig.show()\n",
    "\n",
    "    fig = px.scatter(\n",
    "        arousal_tsne,\n",
    "        x=0,\n",
    "        y=1,\n",
    "        color_discrete_sequence=px.colors.qualitative.G10,\n",
    "        color=a_y_km,\n",
    "        labels={\"color\": \"cluster\"},\n",
    "        hover_data={\"song_id\": dataframe.SongId.unique()},\n",
    "        title=\"Arousal Embeddings\"\n",
    "    )\n",
    "    fig.update_traces(marker_size=8)\n",
    "    fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This block calculates the cronbach alpha parameter for Valence/Arousal.\n",
    "The first experiments is on the original dataset, the second one is on the dataset without outliers.\n",
    "\"\"\"\n",
    "\n",
    "valence_cronbach_alpha = clac_cronbach_alpha(data_frame=valence_data_converter.encoded_df)\n",
    "arousal_cronbach_alpha = clac_cronbach_alpha(data_frame=arousal_data_converter.encoded_df)\n",
    "\n",
    "print(f\"valence_cronbach_alpha with outliers: {Fore.GREEN}{valence_cronbach_alpha}{Style.RESET_ALL}\")\n",
    "print(f\"arousal_cronbach_alpha with outliers: {Fore.GREEN}{arousal_cronbach_alpha}{Style.RESET_ALL}\")\n",
    "\n",
    "valence_df_without_outliers_data_converter = DataConverter(original_df=valence_df_without_outliers)\n",
    "arousal_df_without_outliers_data_converter = DataConverter(original_df=arousal_df_without_outliers)\n",
    "\n",
    "valence_cronbach_alpha = clac_cronbach_alpha(data_frame=valence_df_without_outliers_data_converter.encoded_df)\n",
    "arousal_cronbach_alpha = clac_cronbach_alpha(data_frame=arousal_df_without_outliers_data_converter.encoded_df)\n",
    "\n",
    "print(f\"valence_cronbach_alpha {Fore.BLUE}without outliers{Style.RESET_ALL}: {Fore.GREEN}{valence_cronbach_alpha}{Style.RESET_ALL}\")\n",
    "print(f\"arousal_cronbach_alpha {Fore.BLUE}without outliers{Style.RESET_ALL}: {Fore.GREEN}{arousal_cronbach_alpha}{Style.RESET_ALL}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "data": {
      "text/plain": "                            user_id  item_id    rating\n0  6010bbc8e7ef4b21fa38f9c3a9754ef3        2 -2.323776\n1  3c888e77b992ae3cd2adfe16774e23b9        2 -0.615398\n2  2afd218c3aecb6828d2be327f8b9c46f        2 -1.437036\n3  fd5b08ce362d855ca9152a894348130c        2 -0.822231\n4  9c8073214a052e414811b76012df8847        2 -2.641157",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>item_id</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6010bbc8e7ef4b21fa38f9c3a9754ef3</td>\n      <td>2</td>\n      <td>-2.323776</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3c888e77b992ae3cd2adfe16774e23b9</td>\n      <td>2</td>\n      <td>-0.615398</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2afd218c3aecb6828d2be327f8b9c46f</td>\n      <td>2</td>\n      <td>-1.437036</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>fd5b08ce362d855ca9152a894348130c</td>\n      <td>2</td>\n      <td>-0.822231</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9c8073214a052e414811b76012df8847</td>\n      <td>2</td>\n      <td>-2.641157</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_after_mf = to_mf_df()\n",
    "\n",
    "valence_df_after_mf = df_after_mf[[\"workerID\", \"SongId\", \"Valence\"]]\n",
    "valence_df_after_mf.columns = [\"user_id\", \"item_id\", \"rating\"]\n",
    "\n",
    "arousal_df_after_mf = df_after_mf[[\"workerID\", \"SongId\", \"Arousal\"]]\n",
    "arousal_df_after_mf.columns = [\"user_id\", \"item_id\", \"rating\"]\n",
    "\n",
    "outliers_names = combined_outliers.keys()\n",
    "arousal_df_after_mf.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/17464 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9fba808efdc648929dd292e210c14456"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/17464 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "09a0213db0524171b30a244f8785366e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data consistency with outliers according to direct calculation is: \u001B[33m4.707345624410664e-14\u001B[32m\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16429 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c68113779d54c96bcf2306abdfb68df"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16429 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "840e7dc88a5c4e5b9f0dda07f513c2b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data consistency without outliers according to direct calculation is: \u001B[43m-1.1546319456101628e-14\u001B[42m\n"
     ]
    }
   ],
   "source": [
    "valence_consistency = direct_consistency_calculation(data_frame=valence_df_after_mf)\n",
    "arousal_consistency = direct_consistency_calculation(data_frame=arousal_df_after_mf)\n",
    "\n",
    "print(f\"Raw data consistency with outliers according to direct calculation is: \\x1b[33m{valence_consistency + arousal_consistency}\\x1b[32m\")\n",
    "\n",
    "valence_df_without_outliers = valence_df_after_mf[~valence_df_after_mf.user_id.isin(outliers_names)]\n",
    "arousal_df_without_outliers = arousal_df_after_mf[~arousal_df_after_mf.user_id.isin(outliers_names)]\n",
    "\n",
    "valence_consistency = direct_consistency_calculation(data_frame=valence_df_without_outliers)\n",
    "arousal_consistency = direct_consistency_calculation(data_frame=arousal_df_without_outliers)\n",
    "\n",
    "print(f\"Raw data consistency without outliers according to direct calculation is: \\x1b[43m{valence_consistency + arousal_consistency}\\x1b[42m\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# k_means(dataframe=original_df, title=\"Original Dataset\")\n",
    "k_means(dataframe=df_after_mf, title=\"After MF Dataset\")\n",
    "# plot_embeddings(dataframe=df_after_mf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# This block plot the rows that their emotion was changed after mf.\n",
    "# \"\"\"\n",
    "# mask = ((original_df[\"Valence\"] == df_after_mf[\"Valence\"]) & (original_df[\"Arousal\"] == df_after_mf[\"Arousal\"]))\n",
    "# changes = original_df[mask].copy()\n",
    "# changes[\"New Valence\"] = df_after_mf.Valence\n",
    "# changes[\"New Arousal\"] = df_after_mf.Arousal\n",
    "# print(f\"Number of hits: {len(changes)} / {len(original_df)}\")\n",
    "# print(f\"Hit ratio: {(len(changes) / len(original_df)) * 100}\")\n",
    "# changes.head(len(changes))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}